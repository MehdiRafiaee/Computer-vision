{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehdiRafiaee/Computer-vision/blob/main/Mask_wearing_with_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ed4Ay2tIqOR"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input, Dense, Conv2D, PReLU, Flatten\n",
        "from keras import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import categorical_crossentropy\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from keras.regularizers import l2\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn5f1EzyI3dz"
      },
      "outputs": [],
      "source": [
        "image_cw = glob.glob('G:/MoA8AoM/Lessons/Master/Term3/AI/Project/Moeen/Report2/AI/Proj/IMAGGES/cw/*')\n",
        "image_iw = glob.glob('G:/MoA8AoM/Lessons/Master/Term3/AI/Project/Moeen/Report2/AI/Proj/IMAGGES/iw/*')\n",
        "image_nw = glob.glob('G:/MoA8AoM/Lessons/Master/Term3/AI/Project/Moeen/Report2/AI/Proj/IMAGGES/nw/*')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_sQqNm9JR8u"
      },
      "source": [
        "Resizing Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj2jjNYEJDy6"
      },
      "outputs": [],
      "source": [
        "max_height = 0\n",
        "max_width = 0\n",
        "x_temp_1 = []\n",
        "y = []\n",
        "\n",
        "# Resizing images to the same size\n",
        "for file in image_cw:\n",
        "    image = cv2.imread(file, 0)\n",
        "    height = image.shape[0] // 3\n",
        "    width = image.shape[1] // 3\n",
        "    max_height = np.max((max_height, height))\n",
        "    max_width = np.max((max_width, width))\n",
        "    image = cv2.resize(image, (width, height))\n",
        "    x_temp_1.append(image / 255)\n",
        "    label = 'cw'\n",
        "    y.append(label)\n",
        "\n",
        "for file in image_iw:\n",
        "    image = cv2.imread(file, 0)\n",
        "    height = image.shape[0] // 3\n",
        "    width = image.shape[1] // 3\n",
        "    max_height = np.max((max_height, height))\n",
        "    max_width = np.max((max_width, width))\n",
        "    image = cv2.resize(image, (width, height))\n",
        "    x_temp_1.append(image / 255)\n",
        "    label = 'iw'\n",
        "    y.append(label)\n",
        "\n",
        "for file in image_nw:\n",
        "    image = cv2.imread(file, 0)\n",
        "    height = image.shape[0] // 3\n",
        "    width = image.shape[1] // 3\n",
        "    max_height = np.max((max_height, height))\n",
        "    max_width = np.max((max_width, width))\n",
        "    image = cv2.resize(image, (width, height))\n",
        "    x_temp_1.append(image / 255)\n",
        "    label = 'nw'\n",
        "    y.append(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZCP2bX3JYom"
      },
      "source": [
        "**Removing Extra Space**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_hTFVlyJo3d"
      },
      "outputs": [],
      "source": [
        "# Removing extra spaces from images\n",
        "count = len(x_temp_1)\n",
        "x_temp_2 = np.ones([count, max_height, max_width])\n",
        "for i in range(count):\n",
        "    image = x_temp_1[i]\n",
        "    height = image.shape[0]\n",
        "    width = image.shape[1]\n",
        "    h_start = (max_height - height) // 2\n",
        "    w_start = (max_width - width) // 2\n",
        "    x_temp_2[i, h_start:h_start + height, w_start:w_start + width] = image\n",
        "\n",
        "x = x_temp_2[:, :, :, np.newaxis]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-u1aFKAKB2O"
      },
      "source": [
        "# Test and validation splitting\n",
        "**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOKXr6biKGHt",
        "outputId": "e0ef12e0-20db-478c-c49d-8a9a87cb9dda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e9bc1c373da7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_nc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_nc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_nc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_nc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_nc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ],
      "source": [
        "x, x_test, y, y_test_nc = train_test_split(x, y, test_size=0.2)\n",
        "x_train, x_val, y_train_nc, y_val_nc = train_test_split(x, y, test_size=0.1)\n",
        "\n",
        "y_train = to_categorical(y_train_nc)\n",
        "y_val = to_categorical(y_val_nc)\n",
        "y_test = to_categorical(y_test_nc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvrgJ2I0KODW"
      },
      "source": [
        "CNN nueral network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Z-fqgI8Kenf"
      },
      "outputs": [],
      "source": [
        "inp_layer = Input(shape=x_train.shape[1:])\n",
        "layer = Conv2D(filters=4, kernel_size=(10, 10), strides=(1, 1), padding='valid', data_format='channels_last',\n",
        "               dilation_rate=(1, 1), groups=1, activation='linear', kernel_regularizer=l2(0.03))(inp_layer)\n",
        "layer = PReLU()(layer)\n",
        "layer = Conv2D(filters=8, kernel_size=8, activation='linear', kernel_regularizer=l2(0.03))(layer)\n",
        "layer = PReLU()(layer)\n",
        "layer = Conv2D(filters=16, kernel_size=6, activation='linear', kernel_regularizer=l2(0.03))(layer)\n",
        "layer = PReLU()(layer)\n",
        "layer = Conv2D(filters=32, kernel_size=4, activation='linear', kernel_regularizer=l2(0.03))(layer)\n",
        "layer = PReLU()(layer)\n",
        "layer = Flatten()(layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2iTfwheKxV6"
      },
      "source": [
        "Hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyvJxWkHKhQs"
      },
      "outputs": [],
      "source": [
        "layer = Dense(units=1024, activation='linear', kernel_regularizer=l2(0.03))(layer)\n",
        "layer = PReLU()(layer)\n",
        "layer = Dense(units=128, activation='linear', kernel_regularizer=l2(0.03))(layer)\n",
        "layer = PReLU()(layer)\n",
        "\n",
        "out_layer = Dense(units=10, activation='softmax')(layer)\n",
        "\n",
        "my_model = Model(inp_layer, out_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byi2pVpdK7Ib"
      },
      "source": [
        "Starting learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m84_s_t3K01j"
      },
      "outputs": [],
      "source": [
        "my_model.compile(optimizer=Adam(learning_rate=0.0001), loss=categorical_crossentropy, metrics='acc')\n",
        "results = my_model.fit(x=x_train, y=y_train, batch_size=256, epochs=400, validation_data=(x_val, y_val))\n",
        "\n",
        "print('\\nTest Results:')\n",
        "test_result = my_model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu86pp65LAUS"
      },
      "source": [
        "Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPG-19ajK6bU"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(results.history['loss'])\n",
        "plt.plot(results.history['val_loss'])\n",
        "plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Losses')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvpWQaJoLUjU"
      },
      "source": [
        "Accuracy plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQzR2kC9LTVC"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(results.history['acc'])\n",
        "plt.plot(results.history['val_acc'])\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracies')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzKnmpOaMs2w"
      },
      "source": [
        "Training data confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgPYxfgqLPtS"
      },
      "outputs": [],
      "source": [
        "train_pred_raw = my_model.predict(x_train)\n",
        "train_pred = np.empty(train_pred_raw.shape[0])\n",
        "for i in range(train_pred_raw.shape[0]):\n",
        "    train_pred[i] = np.argmax(train_pred_raw[i, :])\n",
        "cm_train = confusion_matrix(y_train_nc, train_pred)\n",
        "print('\\nTraining Data Confusion Matrix:')\n",
        "print(cm_train)\n",
        "cr_train = classification_report(y_train_nc, train_pred)\n",
        "print('\\nTraining Data Classification Report:')\n",
        "print(cr_train)\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(cm_train, cmap='Blues')\n",
        "num_classes = y_train.shape[1]\n",
        "for i in range(num_classes):\n",
        "    for j in range(num_classes):\n",
        "        plt.text(i, j, cm_train[j, i], fontdict={'color': 'red'})\n",
        "plt.xticks(range(num_classes))\n",
        "plt.yticks(range(num_classes))\n",
        "plt.title('Training Data Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf7rYua9M2_9"
      },
      "source": [
        "Test Data Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FimNzUzZM90o"
      },
      "outputs": [],
      "source": [
        "test_pred_raw = my_model.predict(x_test)\n",
        "test_pred = np.empty(test_pred_raw.shape[0])\n",
        "for i in range(test_pred_raw.shape[0]):\n",
        "    test_pred[i] = np.argmax(test_pred_raw[i, :])\n",
        "cm_test = confusion_matrix(y_test_nc, test_pred)\n",
        "print('\\nTest Data Confusion Matrix:')\n",
        "print(cm_test)\n",
        "cr_test = classification_report(y_test_nc, test_pred)\n",
        "print('\\nTest Data Classification Report:')\n",
        "print(cr_test)\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(cm_test, cmap='Blues')\n",
        "num_classes = y_test.shape[1]\n",
        "for i in range(num_classes):\n",
        "    for j in range(num_classes):\n",
        "        plt.text(i, j, cm_test[j, i], fontdict={'color': 'red'})\n",
        "plt.xticks(range(num_classes))\n",
        "plt.yticks(range(num_classes))\n",
        "plt.title('Test Data Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYQbPPzRtaf6jTqqUgdvat",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}